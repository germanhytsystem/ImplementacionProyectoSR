# Import libraries
import numpy as np
import pandas as pd

# Reading ratings file
ratings = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/dataset/ratings.csv',nrows=10000, sep=',', encoding='latin-1', usecols=['userId','movieId','rating','timestamp'])

# Reading movies file
movies = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/dataset/movies.csv',nrows=10000, sep=',', encoding='latin-1', usecols=['movieId','title','genres'])


# Calcular el número de usuarios únicos
n_usuarios = df_ratings.userId.unique().shape[0]

# Calcular el número de películas únicas
n_peliculas = df_ratings.movieId.unique().shape[0]

# Imprimir el número de usuarios y películas
print('Número de usuarios = ' + str(n_usuarios) + ' | Número de películas = ' + str(n_peliculas))

# Crear una matriz de calificaciones donde los índices son los IDs de usuarios y las columnas son los IDs de películas, con los valores de las calificaciones
matriz_calificaciones = df_ratings.pivot(index='userId', columns='movieId', values='rating').fillna(0)

# Mostrar las primeras filas de la matriz de calificaciones
matriz_calificaciones.head()

# Convertir la matriz de calificaciones a un array numpy
R = matriz_calificaciones.to_numpy()

# Calcular la media de las calificaciones por usuario (promedio de cada fila)
media_calificaciones_usuario = np.mean(R, axis=1)

# Imprimir el tamaño del array de medias de calificaciones por usuario
print(media_calificaciones_usuario.size)

# Restar la media de calificaciones de cada usuario a sus respectivas calificaciones
calificaciones_sin_media = R - media_calificaciones_usuario.reshape(-1, 1)  # Hacer que media_calificaciones_usuario sea vertical mediante reshape


# Calcular el nivel de dispersión del conjunto de datos MovieLens100K
dispersión = round(1.0 - len(ratings) / float(n_usuarios * n_peliculas), 3)

# Imprimir el nivel de dispersión
print('El nivel de dispersión del conjunto de datos MovieLens100K es ' + str(dispersión * 100) + '%')


from scipy.sparse.linalg import svds
# Descomposición en valores singulares (SVD) de la matriz de calificaciones centrada
U, sigma, Vt = svds(calificaciones_sin_media, k = 50)
# Imprimir el tamaño de sigma
print('Size o tamaño de sigma: ', sigma.size)
# Convertir sigma a una matriz diagonal
sigma = np.diag(sigma)
# Imprimir las dimensiones del sigma
print('Shape of sigma: ', sigma.shape)
print(sigma)
# Imprimir las dimensiones del sigma U y Vt
print('Shape of U: ', U.shape)
print('Shape of Vt: ', Vt.shape)
# Calcular las calificaciones predichas para todos los usuarios 
calificaciones_predichas_usuarios = np.dot(np.dot(U, sigma), Vt) + media_calificaciones_usuario.reshape(-1, 1)
print('Calificaciones predichas para todos los usuarios: ', calificaciones_predichas_usuarios.shape)
# Crear un DataFrame de las calificaciones predichas para todos los usuarios y películas
preds = pd.DataFrame(calificaciones_predichas_usuarios, columns=matriz_calificaciones.columns)
preds.head()
def recomendar_peliculas(predicciones, id_usuario, peliculas, calificaciones_originales, num_recomendaciones):
    """
    Implementación para recomendar películas basadas en las predicciones de SVD.
    :param predicciones: La matriz reconstruida por SVD,
    id_usuario: ID del usuario para el cual se desean predecir las películas mejor valoradas,
    peliculas: DataFrame con los datos de las películas,
    calificaciones_originales: Matriz de calificaciones original,
    num_recomendaciones: número de recomendaciones a devolver
    :return: películas mejor valoradas para num_recomendaciones
    """
    # Obtener y ordenar las predicciones del usuario
    numero_fila_usuario = id_usuario - 1  # El ID de usuario empieza en 1, no en 0
    predicciones_usuario_ordenadas = predicciones.iloc[numero_fila_usuario].sort_values(ascending=False)

    # Obtener los datos del usuario y combinar con la información de las películas
    datos_usuario = calificaciones_originales[calificaciones_originales.userId == id_usuario]
    usuario_completo = (datos_usuario.merge(peliculas, how='left', left_on='movieId', right_on='movieId').
                        sort_values(['rating'], ascending=False)
                        )

    print('El usuario {0} ya ha calificado {1} películas.'.format(id_usuario, usuario_completo.shape[0]))
    print('Recomendando las {0} películas con las calificaciones predichas más altas que aún no ha calificado.'.format(num_recomendaciones))

    # Recomendar las películas con las calificaciones predichas más altas que el usuario aún no ha visto
    recomendaciones = (peliculas[~peliculas['movieId'].isin(usuario_completo['movieId'])].
                       merge(pd.DataFrame(predicciones_usuario_ordenadas).reset_index(), how='left',
                             left_on='movieId',
                             right_on='movieId').
                       rename(columns={numero_fila_usuario: 'Predictions'}).
                       sort_values('Predictions', ascending=False).
                       iloc[:num_recomendaciones, :-1]
                       )

    return usuario_completo, recomendaciones
ya_calificadas, predicciones = recomendar_peliculas(preds, 99, movies, ratings, 20)
# Las 20 películas principales que el usuario 1310 ha calificado
ya_calificadas.head(20)
# Las 20 películas principales que esperamos que el usuario 1310 disfrute
predicciones


!pip install scikit-surprise
!pip install --upgrade surprise
!pip show surprise | grep Requires
# Import libraries from Surprise package
from surprise import Reader, Dataset, SVD, Trainset
from surprise.model_selection import cross_validate, KFold
# Cargar la biblioteca Reader
lector = Reader()
# Cargar el conjunto de datos de calificaciones con la biblioteca Dataset
data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], lector)
# Dividir el conjunto de datos para evaluación cruzada de 5 pliegues
# Definir un iterador de validación cruzada
kf = KFold(n_splits=5)
# Utilizar el algoritmo SVD
svd = SVD()
# Luego utilizar kf en la función cross_validate
cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=kf, verbose=True)
# Dividir los datos en conjunto de entrenamiento 
trainset = data.build_full_trainset()
# Entrenar el modelo con el conjunto de entrenamiento
# svd.train(trainset)
svd.fit(trainset)
# Obtener las calificaciones de las películas para el usuario con ID 99
ratings[ratings['userId'] == 99]
svd.predict(99, 1994)
svd.predict(99, 100)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)

from surprise import SVDpp
from surprise import SVD
from surprise import Dataset
from surprise import accuracy
from surprise.model_selection import train_test_split
from surprise.model_selection import GridSearchCV
from surprise.model_selection import cross_validate
# Use movielens-100K
# Dataset.load_builtin('ml-100k') descarga automáticamente el conjunto de datos MovieLens-100K si no está presente localmente
data = Dataset.load_builtin('ml-100k')
trainset, testset = train_test_split(data, test_size=.15)

type(data)  # Devuelve el tipo de objeto de data, que es específico de surprise y contiene el conjunto de datos cargado



